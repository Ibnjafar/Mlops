{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition using Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load HAARCASCADE face classifier\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load functions\n",
    "def face_extractor(img):\n",
    "    # Function detects faces and returns the cropped face\n",
    "    # If no face detected, it returns the input image\n",
    "    \n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    # Crop all faces found\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_face\n",
    "\n",
    "# Initialize Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "\n",
    "# Collect required samples of your face from webcam input (train=960,valid=240)\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if face_extractor(frame) is not None:\n",
    "        count += 1\n",
    "        face = cv2.resize(face_extractor(frame), (200, 200))\n",
    "        # Save file in specified directory with unique name\n",
    "        #change directoy name for person and separate folder for train and valid\n",
    "        file_name_path = '/folder/to/save/images' + str(count) + '.jpg' \n",
    "        cv2.imwrite(file_name_path, face)\n",
    "\n",
    "        # Put count on images and display live count\n",
    "        cv2.putText(face, str(count), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.imshow('Face Cropper', face)\n",
    "        \n",
    "    else:\n",
    "        print(\"Face not found\")\n",
    "        pass\n",
    "\n",
    "    if cv2.waitKey(1) == 13 or count == 960: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      \n",
    "print(\"Collecting Samples Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing VGG16 Model and freezing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "#Resizing images to 64 * 64 pixels\n",
    "\n",
    "img_rows = 64\n",
    "img_cols = 64\n",
    "vgg16 = VGG16(weights = 'imagenet',\n",
    "             include_top = False,\n",
    "             input_shape = (img_rows,img_cols, 3))\n",
    "\n",
    "#Freezing layers\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "# Printing Layers\n",
    "for (i, layer) in enumerate(vgg16.layers):\n",
    "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adding a Top model (Fully connected Deep Learning Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTopModel(bottom_model , num_class, du=256):\n",
    "    \"\"\" creating top models that will be placed on top of the bottom model \"\"\"\n",
    "    top_model = bottom_model.output\n",
    "    top_model = GlobalAveragePooling2D()(top_model)\n",
    "    top_model = Dense(du,activation = \"relu\")(top_model)\n",
    "    top_model = Dense(512,activation = \"relu\")(top_model)\n",
    "    top_model = Dense(1024,activation = \"relu\")(top_model)\n",
    "    top_model = Dropout(0.3)(top_model)\n",
    "    top_model = Dense(num_class, activation = \"softmax\")(top_model)\n",
    "    return top_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Augmentation to increase dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data_dir = \"/root/facedata/train\"\n",
    "validation_data_dir = \"/root/facedata/valid\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                  rotation_range = 30,\n",
    "                                  width_shift_range = 0.2,\n",
    "                                   height_shift_range= 0.2,\n",
    "                                   horizontal_flip= True,\n",
    "                                   fill_mode='nearest')\n",
    "valid_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Specify batch size according to system RAM\n",
    "train_batchsize = 10 \n",
    "valid_batchsize = 10\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                             target_size=(img_rows,img_cols),\n",
    "                                             batch_size=train_batchsize,\n",
    "                                             class_mode= 'categorical')\n",
    "\n",
    "valid_gen = train_datagen.flow_from_directory(validation_data_dir,\n",
    "                                             target_size=(img_rows,img_cols),\n",
    "                                             batch_size=valid_batchsize,\n",
    "                                             class_mode= 'categorical',\n",
    "                                             shuffle=False)\n",
    "num_class = 5\n",
    "\n",
    "model_head= addTopModel(vgg16,num_class)\n",
    "model = Model(inputs=vgg16.input, outputs=model_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"Face_Recognition.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "earlystop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                              factor=0.2,\n",
    "                              patience=5, \n",
    "                              min_lr=0.00001)\n",
    "callbacks = [earlystop,checkpoint,reduce_lr]\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "             optimizer = RMSprop(lr = 0.0001),\n",
    "             metrics = ['accuracy'])\n",
    "train_sample = 960\n",
    "valid_sample = 240\n",
    "epochs = 25\n",
    "batch_size = 10\n",
    "\n",
    "history =model.fit_generator(train_gen,\n",
    "                            steps_per_epoch = train_sample // batch_size,\n",
    "                            epochs = epochs,\n",
    "                            callbacks = callbacks,\n",
    "                            validation_data = valid_gen,\n",
    "                            validation_steps = valid_sample // batch_size)\n",
    "model.save(\"Face_Recognition.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing Model in Real Time using webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "classifier = load_model(\"Face_Recognition.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceCascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30),\n",
    "        flags = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    )\n",
    "\n",
    "    # Draw a rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        img=frame[y:y+h, x:x+w]\n",
    "    #preprocess\n",
    "        #img = image.load_img(img,target_size=(64,64,3))\n",
    "        img = cv2.resize(img,(64,64))\n",
    "        img = image.img_to_array(img)\n",
    "        img = np.expand_dims(img,axis=0)\n",
    "        result = np.argmax(classifier.predict(img,1,verbose=0),axis=1)\n",
    "\n",
    "    # Display the resulting frame {0:'Person 1', 1:'Person 2', 2:'Person 3', 3:'Person 3', 4:'Person 5'}\n",
    "    if result == 0:\n",
    "        person = \"Person 1\"\n",
    "    elif result == 1:\n",
    "        person = \"Person 2\"\n",
    "    elif result == 2 :\n",
    "        person == \"Person 3\"\n",
    "    elif result== 3:\n",
    "        person = \"Person 4\"\n",
    "    elif result == 4:\n",
    "        person = \"Person 5\"\n",
    "    cv2.putText(frame,str(person) , (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

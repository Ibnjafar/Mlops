{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition using Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load HAARCASCADE face classifier\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load functions\n",
    "def face_extractor(img):\n",
    "    # Function detects faces and returns the cropped face\n",
    "    # If no face detected, it returns the input image\n",
    "    \n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    # Crop all faces found\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_face\n",
    "\n",
    "# Initialize Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "\n",
    "# Collect required samples of your face from webcam input (train=960,valid=240)\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if face_extractor(frame) is not None:\n",
    "        count += 1\n",
    "        face = cv2.resize(face_extractor(frame), (200, 200))\n",
    "        # Save file in specified directory with unique name\n",
    "        #change directoy name for person and separate folder for train and valid\n",
    "        file_name_path = '/folder/to/save/images' + str(count) + '.jpg' \n",
    "        cv2.imwrite(file_name_path, face)\n",
    "\n",
    "        # Put count on images and display live count\n",
    "        cv2.putText(face, str(count), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.imshow('Face Cropper', face)\n",
    "        \n",
    "    else:\n",
    "        print(\"Face not found\")\n",
    "        pass\n",
    "\n",
    "    if cv2.waitKey(1) == 13 or count == 960: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      \n",
    "print(\"Collecting Samples Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing VGG16 Model and freezing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "#Resizing images to 64 * 64 pixels\n",
    "\n",
    "img_rows = 64\n",
    "img_cols = 64\n",
    "vgg16 = VGG16(weights = 'imagenet',\n",
    "             include_top = False,\n",
    "             input_shape = (img_rows,img_cols, 3))\n",
    "\n",
    "#Freezing layers\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "# Printing Layers\n",
    "for (i, layer) in enumerate(vgg16.layers):\n",
    "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adding a Top model (Fully connected Deep Learning Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTopModel(bottom_model , num_class, du=256):\n",
    "    \"\"\" creating top models that will be placed on top of the bottom model \"\"\"\n",
    "    top_model = bottom_model.output\n",
    "    top_model = GlobalAveragePooling2D()(top_model)\n",
    "    top_model = Dense(du,activation = \"relu\")(top_model)\n",
    "    top_model = Dense(512,activation = \"relu\")(top_model)\n",
    "    top_model = Dense(1024,activation = \"relu\")(top_model)\n",
    "    top_model = Dropout(0.3)(top_model)\n",
    "    top_model = Dense(num_class, activation = \"softmax\")(top_model)\n",
    "    return top_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Augmentation to increase dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4800 images belonging to 5 classes.\n",
      "Found 1200 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data_dir = \"/root/facedata/train\"\n",
    "validation_data_dir = \"/root/facedata/valid\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                  rotation_range = 30,\n",
    "                                  width_shift_range = 0.2,\n",
    "                                   height_shift_range= 0.2,\n",
    "                                   horizontal_flip= True,\n",
    "                                   fill_mode='nearest')\n",
    "valid_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Specify batch size according to system RAM\n",
    "train_batchsize = 10 \n",
    "valid_batchsize = 10\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                             target_size=(img_rows,img_cols),\n",
    "                                             batch_size=train_batchsize,\n",
    "                                             class_mode= 'categorical')\n",
    "\n",
    "valid_gen = train_datagen.flow_from_directory(validation_data_dir,\n",
    "                                             target_size=(img_rows,img_cols),\n",
    "                                             batch_size=valid_batchsize,\n",
    "                                             class_mode= 'categorical',\n",
    "                                             shuffle=False)\n",
    "num_class = 5\n",
    "\n",
    "model_head= addTopModel(vgg16,num_class)\n",
    "model = Model(inputs=vgg16.input, outputs=model_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 15,508,037\n",
      "Trainable params: 793,349\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-9a18b00a7d0f>:31: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/25\n",
      "96/96 [==============================] - ETA: 0s - loss: 1.3980 - accuracy: 0.4667\n",
      "Epoch 00001: saving model to Face_Recognition.h5\n",
      "96/96 [==============================] - 30s 310ms/step - loss: 1.3980 - accuracy: 0.4667 - val_loss: 1.1310 - val_accuracy: 0.7625 - lr: 1.0000e-04\n",
      "Epoch 2/25\n",
      "96/96 [==============================] - ETA: 0s - loss: 1.0021 - accuracy: 0.7052\n",
      "Epoch 00002: saving model to Face_Recognition.h5\n",
      "96/96 [==============================] - 29s 307ms/step - loss: 1.0021 - accuracy: 0.7052 - val_loss: 0.9752 - val_accuracy: 0.8167 - lr: 1.0000e-04\n",
      "Epoch 3/25\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.6986 - accuracy: 0.8021\n",
      "Epoch 00003: saving model to Face_Recognition.h5\n",
      "96/96 [==============================] - 30s 313ms/step - loss: 0.6986 - accuracy: 0.8021 - val_loss: 1.5307 - val_accuracy: 0.4667 - lr: 1.0000e-04\n",
      "Epoch 4/25\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.4804 - accuracy: 0.8625\n",
      "Epoch 00004: saving model to Face_Recognition.h5\n",
      "96/96 [==============================] - 30s 316ms/step - loss: 0.4804 - accuracy: 0.8625 - val_loss: 0.4427 - val_accuracy: 0.9250 - lr: 1.0000e-04\n",
      "Epoch 5/25\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.3362 - accuracy: 0.9031\n",
      "Epoch 00005: saving model to Face_Recognition.h5\n",
      "96/96 [==============================] - 29s 307ms/step - loss: 0.3362 - accuracy: 0.9031 - val_loss: 0.4046 - val_accuracy: 0.8792 - lr: 1.0000e-04\n",
      "Epoch 6/25\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.2687 - accuracy: 0.9333\n",
      "Epoch 00006: saving model to Face_Recognition.h5\n",
      "96/96 [==============================] - 29s 306ms/step - loss: 0.2687 - accuracy: 0.9333 - val_loss: 0.2428 - val_accuracy: 0.9292 - lr: 1.0000e-04\n",
      "Epoch 7/25\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.2341 - accuracy: 0.9229\n",
      "Epoch 00007: saving model to Face_Recognition.h5\n",
      "96/96 [==============================] - 30s 309ms/step - loss: 0.2341 - accuracy: 0.9229 - val_loss: 0.4167 - val_accuracy: 0.8667 - lr: 1.0000e-04\n",
      "Epoch 8/25\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.1700 - accuracy: 0.9542\n",
      "Epoch 00008: saving model to Face_Recognition.h5\n",
      "96/96 [==============================] - 30s 312ms/step - loss: 0.1700 - accuracy: 0.9542 - val_loss: 0.4329 - val_accuracy: 0.8708 - lr: 1.0000e-04\n",
      "Epoch 9/25\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.1701 - accuracy: 0.9469\n",
      "Epoch 00009: saving model to Face_Recognition.h5\n",
      "96/96 [==============================] - 30s 310ms/step - loss: 0.1701 - accuracy: 0.9469 - val_loss: 0.1116 - val_accuracy: 0.9750 - lr: 1.0000e-04\n",
      "Epoch 10/25\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.1229 - accuracy: 0.9677\n",
      "Epoch 00010: saving model to Face_Recognition.h5\n",
      "96/96 [==============================] - 29s 302ms/step - loss: 0.1229 - accuracy: 0.9677 - val_loss: 0.1976 - val_accuracy: 0.9292 - lr: 1.0000e-04\n",
      "Epoch 11/25\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.1271 - accuracy: 0.9583\n",
      "Epoch 00011: saving model to Face_Recognition.h5\n",
      "96/96 [==============================] - 30s 308ms/step - loss: 0.1271 - accuracy: 0.9583 - val_loss: 0.2348 - val_accuracy: 0.9208 - lr: 1.0000e-04\n",
      "Epoch 12/25\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.1054 - accuracy: 0.9719\n",
      "Epoch 00012: saving model to Face_Recognition.h5\n",
      "96/96 [==============================] - 29s 297ms/step - loss: 0.1054 - accuracy: 0.9719 - val_loss: 0.2170 - val_accuracy: 0.9125 - lr: 1.0000e-04\n",
      "Epoch 13/25\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.1140 - accuracy: 0.9594\n",
      "Epoch 00013: saving model to Face_Recognition.h5\n",
      "96/96 [==============================] - 29s 303ms/step - loss: 0.1140 - accuracy: 0.9594 - val_loss: 0.3531 - val_accuracy: 0.8583 - lr: 1.0000e-04\n",
      "Epoch 14/25\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.0879 - accuracy: 0.9677Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00014: saving model to Face_Recognition.h5\n",
      "96/96 [==============================] - 30s 309ms/step - loss: 0.0879 - accuracy: 0.9677 - val_loss: 0.1400 - val_accuracy: 0.9417 - lr: 1.0000e-04\n",
      "Epoch 00014: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"Face_Recognition.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "earlystop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                              factor=0.2,\n",
    "                              patience=5, \n",
    "                              min_lr=0.00001)\n",
    "callbacks = [earlystop,checkpoint,reduce_lr]\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "             optimizer = RMSprop(lr = 0.0001),\n",
    "             metrics = ['accuracy'])\n",
    "train_sample = 960\n",
    "valid_sample = 240\n",
    "epochs = 25\n",
    "batch_size = 10\n",
    "\n",
    "history =model.fit_generator(train_gen,\n",
    "                            steps_per_epoch = train_sample // batch_size,\n",
    "                            epochs = epochs,\n",
    "                            callbacks = callbacks,\n",
    "                            validation_data = valid_gen,\n",
    "                            validation_steps = valid_sample // batch_size)\n",
    "model.save(\"Face_Recognition.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing Model in Real Time using webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "classifier = load_model(\"Face_Recognition.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceCascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30),\n",
    "        flags = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    )\n",
    "\n",
    "    # Draw a rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        img=frame[y:y+h, x:x+w]\n",
    "    #preprocess\n",
    "        #img = image.load_img(img,target_size=(64,64,3))\n",
    "        img = cv2.resize(img,(64,64))\n",
    "        img = image.img_to_array(img)\n",
    "        img = np.expand_dims(img,axis=0)\n",
    "        result = np.argmax(classifier.predict(img,1,verbose=0),axis=1)\n",
    "\n",
    "    # Display the resulting frame {0:'Person 1', 1:'Person 2', 2:'Person 3', 3:'Person 3', 4:'Person 5'}\n",
    "    if result == 0:\n",
    "        person = \"Person 1\"\n",
    "    elif result == 1:\n",
    "        person = \"Person 2\"\n",
    "    elif result == 2 :\n",
    "        person == \"Person 3\"\n",
    "    elif result== 3:\n",
    "        person = \"Person 4\"\n",
    "    elif result == 4:\n",
    "        person = \"Person 5\"\n",
    "    cv2.putText(frame,str(person) , (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
